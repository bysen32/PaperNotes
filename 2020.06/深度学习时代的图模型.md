# 深度学习时代的图模型

[原文地址](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754422&idx=4&sn=0dc881487f362322a875b4ce06e645f7&chksm=871a8908b06d001ef7386ccc752827c20711877a4a23d6a8318978095dd241d118257c607b22&scene=21#wechat_redirect)

> 由于深度学习对于**图结构数据**的应用存在问题，图神经网络开始出现。

现有方法分类：

1. 半监督方法：图神经网络和图卷积网络
2. 无监督方法：图自编码器
3. 近期研究：图循环神经网络+图强化学习

![图深度学习方法分类](https://raw.githubusercontent.com/bysen32/PicGo/master/20200531233829.png)

传统深度学习框架应用到图结构存在的挑战：

1. 不规则领域：不规则的图结构
1. 多变的结构和任务：
   1. 图**结构**的多变
      - 同质，异质
      - 加权，不加权
      - 有符号，无符号
   2. 图**任务**的多变
      - 节点问题：节点分类、连接预测
      - 图问题：图分类、图生成
1. 可扩展性和并行化：图结构节点与边互连，需要整体考虑
1. 跨学科：通常与其他学科相关联

## 图神经网络 GNN

思想：用低维状态向量$s_i(i\leq i \leq N)$表示各节点，以编码图的结构信息。
递归定义：
$$s_i = \sum_{j\in\mathcal{N}(i)}\mathcal{F}(s_i,s_j,F^V_i,F^V_j,F^E_{i,j})\tag{1}$$
$\mathcal{F}(\cdot)$为待学习的参数函数
$$\hat{y}_i = \mathcal{O}(s_i,F^V_i)\tag{2}$$
带参函数$\mathcal{O}(\cdot)$获取最终输出

对于图任务，添加一个代表整个图属性的特殊节点。
用半监督的方法学习参数模型：

1. 使用雅各比方法迭代求解Eq.(1)
2. 稳定后使用AlmeidaPineda算法执行一个梯度下降步，以最小化损失函数
3. 重复直至收敛

当前最优的GCN具有Eq.(1)类似的公式，遵循**邻近交换信息框架**。

GNN理论的缺陷：

1. 要确保Eq.(1)有唯一解，$F(\cdot)$必须是压缩映射。
2. 梯度下降需要迭代多次，GNN计算成本高

**重大改进**：门控图序列神经网络GGSNN(Gated Graph Sequence Neural Network)
将Eq.1的递归定义换成了门控循环单元GRU：
$$s^t_i=(1-z^t_i)\odot s^{(t-1)}_i + z^t_i\odot\bar{s}^t_i \tag{3}$$

- [ ] ==GRU公式的物理含义是什么==
   使用一个门限变量$z^t_i$实现过滤掉之前状态中不重要的属性，记忆当前状态中重要的属性,将两部分合并作为当前节点的属性。
